{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "**Note:** Install the *sentence-transformers* package to run this notebook. If you get an error that *sentencepiece* is missing, try restarting and rerunning the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define filepath\n",
    "products_path = \"./data/products_train.csv\"\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def read_product_data():\n",
    "    return pd.read_csv(products_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data = read_product_data()\n",
    "product_data = product_data[product_data[\"locale\"].isin([\"DE\", \"UK\", \"JP\"])]\n",
    "\n",
    "product_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate german, UK and japanese products and drop every column except for id and title\n",
    "products_de = product_data[product_data[\"locale\"] == \"DE\"][[\"id\", \"title\"]]\n",
    "products_uk = product_data[product_data[\"locale\"] == \"UK\"][[\"id\", \"title\"]]\n",
    "products_jp = product_data[product_data[\"locale\"] == \"JP\"][[\"id\", \"title\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models\n",
    "\n",
    "Two transformer models are used to create the embeddings of the product titles:\n",
    "\n",
    "- cross-en-de-roberta-sentence-transformer\n",
    "    - https://huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer\n",
    "    - creates embeddings for german and english texts\n",
    "- luke-japanese-base-lite\n",
    "    - https://huggingface.co/studio-ousia/luke-japanese-base-lite\n",
    "    - creates embeddings for japanese texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_de_model = SentenceTransformer('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_model = SentenceTransformer(\"studio-ousia/luke-japanese-base-lite\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Embeddings (Titles Only)\n",
    "\n",
    "The embeddings are computed in batches and they are stored in three different files (one for each locale). Additionally, three logfiles are created, which store the last successfully processed batch. In case that the notebook crashed or is interrupted for some other reason, the logfile is read when the notebook is restarted and the processing starts after the last successfully processed batch.\n",
    "\n",
    "**Note:** Add two subfolders named *embeddings* and *logfiles* to the data folder to execute the following code cells without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "n_batches_de = len(products_de) // batch_size + 1\n",
    "n_batches_uk = len(products_uk) // batch_size + 1\n",
    "n_batches_jp = len(products_jp) // batch_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "##### GERMAN LOCALE #####\n",
    "#########################\n",
    "\n",
    "# read logfile to look for last successfully processed batch\n",
    "de_logfile_path = 'data/logfiles/de_embeddings_log.txt'\n",
    "try:\n",
    "    f = open(de_logfile_path, \"r\")\n",
    "    prev_batch_idx_de = int(f.read())\n",
    "    f.close()\n",
    "except FileNotFoundError:\n",
    "    prev_batch_idx_de = -1\n",
    "\n",
    "if prev_batch_idx_de < n_batches_de-1:\n",
    "    for batch_idx in range(prev_batch_idx_de+1, n_batches_de):\n",
    "\n",
    "        print(f\"processing batch {batch_idx+1} / {n_batches_de}\")\n",
    "        \n",
    "        # ---read product data of this batch---\n",
    "        first_row = batch_idx * batch_size\n",
    "        last_row = first_row + batch_size\n",
    "        batch_product_data = products_de.iloc[first_row:last_row]\n",
    "\n",
    "        # ---compute embeddings---\n",
    "        emb = en_de_model.encode(batch_product_data[\"title\"].values)\n",
    "\n",
    "        # ---store embeddings in dataframe---\n",
    "        emb_df = pd.DataFrame(emb, index=batch_product_data.index)\n",
    "\n",
    "        # ---write embeddings to csv file---\n",
    "        # Set writing mode to append after first chunk\n",
    "        mode = 'w' if batch_idx == 0 else 'a'\n",
    "        # add header if it is the first chunk\n",
    "        header = batch_idx == 0\n",
    "        # write to file\n",
    "        emb_df.to_csv(\n",
    "            \"data/embeddings/de_embeddings.csv\",\n",
    "            header=header,\n",
    "            mode=mode\n",
    "        )\n",
    "\n",
    "        # write chunk index to log file\n",
    "        f = open(de_logfile_path, \"w\")\n",
    "        f.write(str(batch_idx))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "##### UK LOCALE #####\n",
    "#####################\n",
    "\n",
    "# read logfile to look for last successfully processed batch\n",
    "uk_logfile_path = 'data/logfiles/uk_embeddings_log.txt'\n",
    "try:\n",
    "    f = open(uk_logfile_path, \"r\")\n",
    "    prev_batch_idx_uk = int(f.read())\n",
    "    f.close()\n",
    "except FileNotFoundError:\n",
    "    prev_batch_idx_uk = -1\n",
    "\n",
    "if prev_batch_idx_uk < n_batches_uk-1:\n",
    "    for batch_idx in range(prev_batch_idx_uk+1, n_batches_uk):\n",
    "\n",
    "        print(f\"processing batch {batch_idx+1} / {n_batches_uk}\")\n",
    "        \n",
    "        # ---read product data of this batch---\n",
    "        first_row = batch_idx * batch_size\n",
    "        last_row = first_row + batch_size\n",
    "        batch_product_data = products_uk.iloc[first_row:last_row]\n",
    "\n",
    "        # ---compute embeddings---\n",
    "        emb = en_de_model.encode(batch_product_data[\"title\"].values)\n",
    "\n",
    "        # ---store embeddings in dataframe---\n",
    "        emb_df = pd.DataFrame(emb, index=batch_product_data.index)\n",
    "\n",
    "        # ---write embeddings to csv file---\n",
    "        # Set writing mode to append after first chunk\n",
    "        mode = 'w' if batch_idx == 0 else 'a'\n",
    "        # add header if it is the first chunk\n",
    "        header = batch_idx == 0\n",
    "        # write to file\n",
    "        emb_df.to_csv(\n",
    "            \"data/embeddings/uk_embeddings.csv\",\n",
    "            header=header,\n",
    "            mode=mode\n",
    "        )\n",
    "\n",
    "        # write chunk index to log file\n",
    "        f = open(uk_logfile_path, \"w\")\n",
    "        f.write(str(batch_idx))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "##### JAPAN LOCALE #####\n",
    "########################\n",
    "\n",
    "# read logfile to look for last successfully processed batch\n",
    "jp_logfile_path = 'data/logfiles/jp_embeddings_log.txt'\n",
    "try:\n",
    "    f = open(jp_logfile_path, \"r\")\n",
    "    prev_batch_idx_jp = int(f.read())\n",
    "    f.close()\n",
    "except FileNotFoundError:\n",
    "    prev_batch_idx_jp = -1\n",
    "\n",
    "if prev_batch_idx_jp < n_batches_jp-1:\n",
    "    for batch_idx in range(prev_batch_idx_jp+1, n_batches_jp):\n",
    "\n",
    "        print(f\"processing batch {batch_idx+1} / {n_batches_jp}\")\n",
    "        \n",
    "        # ---read product data of this batch---\n",
    "        first_row = batch_idx * batch_size\n",
    "        last_row = first_row + batch_size\n",
    "        batch_product_data = products_jp.iloc[first_row:last_row]\n",
    "\n",
    "        # ---compute embeddings---\n",
    "        emb = jp_model.encode(batch_product_data[\"title\"].values)\n",
    "\n",
    "        # ---store embeddings in dataframe---\n",
    "        emb_df = pd.DataFrame(emb, index=batch_product_data.index)\n",
    "\n",
    "        # ---write embeddings to csv file---\n",
    "        # Set writing mode to append after first chunk\n",
    "        mode = 'w' if batch_idx == 0 else 'a'\n",
    "        # add header if it is the first chunk\n",
    "        header = batch_idx == 0\n",
    "        # write to file\n",
    "        emb_df.to_csv(\n",
    "            \"data/embeddings/jp_embeddings.csv\",\n",
    "            header=header,\n",
    "            mode=mode\n",
    "        )\n",
    "\n",
    "        # write chunk index to log file\n",
    "        f = open(jp_logfile_path, \"w\")\n",
    "        f.write(str(batch_idx))\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm2_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
